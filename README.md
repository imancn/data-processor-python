# Data Processing Framework

A **production-ready, domain-agnostic data processing platform** built with Python and ClickHouse. This framework provides a comprehensive foundation for building robust ETL/ELT pipelines with enterprise-grade features including automatic pipeline discovery, idempotent deployment, comprehensive testing, and operational monitoring.

## ğŸŒŸ Why This Framework?

- **ğŸ¯ Zero Business Logic**: Completely generic - works for any data processing use case
- **ğŸ”„ Auto-Discovery**: Just add your pipeline files - framework handles the rest
- **ğŸš€ Production Ready**: Idempotent deployment, process management, health monitoring
- **ğŸ§ª Test-Driven**: Comprehensive test suite with coverage reporting
- **ğŸ“Š Observable**: Structured logging, monitoring, and operational insights
- **âš™ï¸ Operational**: Complete toolkit for development, deployment, and maintenance
- **ğŸ”§ Extensible**: Plugin architecture for extractors, transformers, and loaders
- **ğŸ“ˆ Scalable**: Designed for enterprise data processing workloads

## ğŸ† Enterprise Features

### **Deployment & Operations**
- âœ… **Idempotent Deployment**: Safe, repeatable deployments with automatic cleanup
- âœ… **Process Management**: Automatic process lifecycle and cron job management  
- âœ… **Health Monitoring**: Post-deployment verification and continuous health checks
- âœ… **Log Management**: Centralized, structured logging with job-specific logs
- âœ… **Migration System**: Version-controlled database schema management
- âœ… **Backfill System**: Historical data processing and monitoring

### **Development & Testing**
- âœ… **Auto-Discovery**: Zero-config pipeline registration and scheduling
- âœ… **Test Framework**: Unit + Integration + E2E tests with coverage reporting
- âœ… **Local Development**: Complete local development environment
- âœ… **CI/CD Ready**: JUnit XML, coverage reports, and automation-friendly
- âœ… **Documentation**: Comprehensive architecture and usage documentation

### **Architecture & Reliability**
- âœ… **Plugin Architecture**: Extensible extractors, transformers, and loaders
- âœ… **Error Handling**: Comprehensive error handling with retry logic
- âœ… **Configuration Management**: Environment-based configuration with validation
- âœ… **Security**: No hardcoded secrets, secure connection management
- âœ… **Performance**: Optimized for high-throughput data processing

## ğŸ“ Project Structure

```
data-processor/                   # ğŸ  Production-Ready Data Processing Platform
â”œâ”€â”€ ğŸ“‹ docs/                     # ğŸ“– Complete Documentation
â”‚   â”œâ”€â”€ architecture/            # ğŸ—ï¸ System Architecture
â”‚   â”œâ”€â”€ diagrams/                # ğŸ“Š Visual Diagrams
â”‚   â”œâ”€â”€ quick-start.md           # ğŸš€ Getting Started Guide
â”‚   â”œâ”€â”€ api-reference.md         # ğŸ“š Complete API Reference
â”‚   â””â”€â”€ developer-guide.md       # ğŸ› ï¸ Development Guidelines
â”œâ”€â”€ ğŸ§  src/                      # ğŸ’» Core Framework Source Code
â”‚   â”œâ”€â”€ core/                   # âš™ï¸ Framework Foundation
â”‚   â”‚   â”œâ”€â”€ config.py          # ğŸ”§ Configuration management & validation
â”‚   â”‚   â”œâ”€â”€ logging.py         # ğŸ“ Multi-level structured logging system
â”‚   â”‚   â”œâ”€â”€ models.py          # ğŸ“Š Pydantic data models
â”‚   â”‚   â”œâ”€â”€ exceptions.py      # âš ï¸ Custom exception hierarchy
â”‚   â”‚   â””â”€â”€ validators.py      # âœ… Data validation utilities
â”‚   â”œâ”€â”€ pipelines/             # ğŸ”„ Pipeline System (ADD YOUR PIPELINES HERE)
â”‚   â”‚   â”œâ”€â”€ tools/             # ğŸ› ï¸ Reusable Pipeline Components
â”‚   â”‚   â”‚   â”œâ”€â”€ extractors/    # ğŸ“¥ Data Sources (HTTP, DB, Files)
â”‚   â”‚   â”‚   â”œâ”€â”€ transformers/  # ğŸ”„ Data Processing
â”‚   â”‚   â”‚   â””â”€â”€ loaders/       # ğŸ“¤ Data Destinations
â”‚   â”‚   â”œâ”€â”€ pipeline_factory.py # ğŸ­ ETL/ELT pipeline creation
â”‚   â”‚   â””â”€â”€ pipeline_registry.py # ğŸ“‹ Pipeline registration system
â”‚   â””â”€â”€ main.py                # ğŸš€ Main application & auto-discovery
â”œâ”€â”€ ğŸ—„ï¸ migrations/              # ğŸ“Š Database Schema Management
â”‚   â”œâ”€â”€ sql/                   # ğŸ“„ Versioned SQL migration files
â”‚   â””â”€â”€ migration_manager.py   # âš™ï¸ Migration orchestration & tracking
â”œâ”€â”€ ğŸ“œ scripts/                 # ğŸ”§ Utility Scripts
â”‚   â”œâ”€â”€ run.py                 # â–¶ï¸ Pipeline runner with job logging
â”‚   â”œâ”€â”€ backfill.py            # â®ï¸ Historical data processing
â”‚   â””â”€â”€ run_tests.py           # ğŸ§ª Comprehensive test runner
â”œâ”€â”€ ğŸ§ª tests/                   # ğŸ”¬ Comprehensive Test Suite
â”‚   â”œâ”€â”€ unit/                  # ğŸ§© Component unit tests
â”‚   â”œâ”€â”€ integration/           # ğŸ”— End-to-end system tests
â”‚   â”œâ”€â”€ performance/           # ğŸ“ˆ Performance and load tests
â”‚   â””â”€â”€ results/               # ğŸ“Š Auto-generated test results
â”œâ”€â”€ ğŸš€ deploy.sh               # ğŸ¯ Idempotent deployment automation
â”œâ”€â”€ âš™ï¸ run.sh                  # ğŸ› ï¸ Local operations toolkit (14 commands)
â”œâ”€â”€ ğŸ“‹ env.example             # ğŸ”§ Environment configuration template
â””â”€â”€ ğŸ“„ requirements.txt        # ğŸ“¦ Python dependencies
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Copy environment template
cp env.example .env

# Edit configuration
vim .env
```

### 2. Install Dependencies

```bash
# Install Python dependencies
pip install -r requirements.txt

# Setup database
./run.sh setup_db
./run.sh migrate
```

### 3. Create Your First Pipeline

Create `src/pipelines/my_pipeline.py`:

```python
import pandas as pd
from pipelines.pipeline_factory import create_etl_pipeline
from pipelines.tools.extractors.http_extractor import create_http_extractor
from pipelines.tools.loaders.console_loader import create_console_loader
from pipelines.tools.transformers.transformers import create_lambda_transformer

# Pipeline registry
PIPELINE_REGISTRY = {}

def create_my_pipeline():
    """Create your custom pipeline."""
    
    # Create extractor
    extractor = create_http_extractor(
        url="https://api.example.com/data",
        headers={"Accept": "application/json"},
        name="My Data Extractor"
    )
    
    # Create transformer
    def transform_data(df):
        df['processed_at'] = pd.Timestamp.now()
        return df
    
    transformer = create_lambda_transformer(transform_data, "My Transformer")
    
    # Create loader
    loader = create_console_loader("My Console Loader")
    
    # Create pipeline
    pipeline = create_etl_pipeline(
        extractor=extractor,
        transformer=transformer,
        loader=loader,
        name="My Pipeline"
    )
    
    return {
        'pipeline': pipeline,
        'description': 'My custom data pipeline',
        'schedule': '0 * * * *',  # Hourly
        'table_name': 'my_data'
    }

def register_pipelines():
    """Register all pipelines in this module."""
    PIPELINE_REGISTRY['my_pipeline'] = create_my_pipeline()

def get_pipeline_registry():
    """Get the pipeline registry."""
    return PIPELINE_REGISTRY

# Auto-register when module is imported
register_pipelines()
```

### 4. Test Your Pipeline

```bash
# List pipelines (should now show your pipeline)
./run.sh list

# Run your pipeline
./run.sh cron_run my_pipeline

# Check logs
tail -f logs/jobs/my_pipeline.log
```

### 5. Deploy to Production

```bash
# Deploy to remote server
./deploy.sh username hostname

# Verify deployment
ssh username@hostname "cd data-processor && ./run.sh list"
```

## ğŸ”§ Available Commands

### Local Operations (`run.sh`)

```bash
./run.sh check                   # Check dependencies
./run.sh test                    # Run all tests
./run.sh cron_run NAME          # Run a pipeline
./run.sh list                   # List available pipelines
./run.sh setup_db               # Setup database
./run.sh drop_db                # Drop and recreate database
./run.sh migrate                # Run database migrations
./run.sh migrate_status         # Show migration status
./run.sh backfill [DAYS] [JOBS] # Backfill historical data
./run.sh backfill_list          # List available jobs
./run.sh backfill_counts        # Show data counts
./run.sh setup_cron NAME        # Setup cron for pipeline
./run.sh kill                   # Kill all running processes
./run.sh clean                  # Clean and restart
./run.sh help                   # Show help
```

### Deployment (`deploy.sh`)

```bash
./deploy.sh user host           # Deploy to server
./deploy.sh user host --clean   # Deploy with cleanup
```

## ğŸ“Š Testing

The framework includes comprehensive testing with detailed reporting:

```bash
# Run all tests
./run.sh test

# Run only unit tests
./run.sh test unit

# Run only integration tests
./run.sh test integration

# Run fast tests (exclude slow tests)
./run.sh test --fast
```

Test results are saved in `tests/results/` with:
- Test execution reports (JSON)
- Code coverage reports (HTML + JSON)
- JUnit XML for CI/CD integration

## ğŸ—ï¸ Architecture

### System Design Philosophy
This framework follows **clean architecture principles** with clear separation of concerns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ¯ Your Pipeline Logic                   â”‚  â† Domain Layer
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ”„ Pipeline Framework (Discovery, Factory, Registry)      â”‚  â† Application Layer
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ› ï¸ Tools (Extractors, Transformers, Loaders)             â”‚  â† Service Layer
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš™ï¸ Core (Config, Logging, Migration, Cron)               â”‚  â† Infrastructure Layer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Architectural Patterns

#### ğŸ” **Auto-Discovery Pattern**
- **Zero Configuration**: Just add `*_pipeline.py` files
- **Dynamic Loading**: Framework discovers and registers automatically
- **Convention Over Configuration**: Follow naming patterns, get functionality

#### ğŸ­ **Factory Pattern**
- **Pipeline Creation**: `create_etl_pipeline()`, `create_el_pipeline()`
- **Component Factory**: Standardized creation of extractors, transformers, loaders
- **Dependency Injection**: Components injected at runtime

#### ğŸ“‹ **Registry Pattern**
- **Central Registration**: All pipelines registered in central registry
- **Runtime Discovery**: Pipelines discovered and registered at startup
- **Scheduling Integration**: Registry automatically creates cron jobs

#### ğŸ”Œ **Plugin Architecture**
- **Extractors**: HTTP, Database, File extractors
- **Transformers**: Lambda, Type, Column transformers  
- **Loaders**: ClickHouse, Console, File loaders
- **Extensible**: Easy to add new components

## ğŸ“ Configuration

Key environment variables in `.env`:

```bash
# ClickHouse Database
CLICKHOUSE_HOST=172.30.63.35
CLICKHOUSE_PORT=8123
CLICKHOUSE_USER=data-processor
CLICKHOUSE_PASSWORD=your_password
CLICKHOUSE_DATABASE=data_warehouse

# Logging
LOG_LEVEL=INFO
LOG_DIR=logs

# Application
TIMEOUT=30
BATCH_SIZE=1000

# Add your pipeline-specific variables here
API_KEY=your_api_key_here
API_BASE_URL=https://api.example.com
```

## ğŸ” Monitoring & Debugging

### Logs

Logs are organized in the `logs/` directory:
- `logs/system/application.log` - System-level logs
- `logs/jobs/{job_name}.log` - Per-job detailed logs
- `logs/cron.log` - Cron execution logs

### Data Monitoring

```bash
# Check data counts in all tables
./run.sh backfill_counts

# List available jobs
./run.sh backfill_list

# Backfill specific jobs
./run.sh backfill 7 job1 job2  # Last 7 days for job1 and job2
```

### Migration Status

```bash
# Check migration status
./run.sh migrate_status

# Run pending migrations
./run.sh migrate
```

## ğŸš€ Production Deployment

1. **Prepare Server**: Ensure Python 3.9+, ClickHouse access, and network connectivity
2. **Configure Environment**: Set up `.env` with production values
3. **Deploy**: `./deploy.sh user hostname`
4. **Verify**: Check logs, data counts, and cron jobs
5. **Monitor**: Set up monitoring for logs and data freshness

### Deployment Checklist

- [ ] Server access and permissions configured
- [ ] ClickHouse database accessible
- [ ] Environment variables configured
- [ ] API keys and external service access verified
- [ ] Network connectivity tested
- [ ] Monitoring and alerting configured

## ğŸ“š Documentation

- **[ğŸ“– Complete Documentation](docs/README.md)** - Comprehensive documentation index
- **[ğŸš€ Quick Start Guide](docs/quick-start.md)** - Get up and running quickly
- **[ğŸ—ï¸ Architecture Overview](docs/architecture/README.md)** - System design and components
- **[ğŸ“Š System Diagrams](docs/diagrams/README.md)** - Visual architecture representations
- **[ğŸ“š API Reference](docs/api-reference.md)** - Complete API documentation
- **[ğŸ› ï¸ Developer Guide](docs/developer-guide.md)** - Contributing and extending

## ğŸ¤ Contributing

1. Add your pipeline modules to `src/pipelines/`
2. Follow the pipeline module pattern (see example above)
3. Add tests in `tests/unit/` and `tests/integration/`
4. Run the test suite: `./run.sh test`
5. Update documentation as needed

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Troubleshooting

### Common Issues

**"No pipelines found"**
- Add pipeline modules to `src/pipelines/` following the naming pattern `*_pipeline.py`

**"Database connection failed"**
- Check ClickHouse configuration in `.env`
- Verify network connectivity and credentials

**"Tests failing"**
- Install test dependencies: `pip install pytest pytest-asyncio`
- Check test results in `tests/results/latest_test_results.json`

**"Deployment fails"**
- Check server access and permissions
- Verify deployment directory is writable
- Check logs in deployment output

### Getting Help

1. Check the logs in `logs/` directory
2. Run tests to verify system health: `./run.sh test`
3. Check configuration: `./run.sh check`
4. Review test results in `tests/results/`

---

## ğŸ‰ Ready to Build Something Amazing?

This framework provides everything you need to build **production-grade data processing systems** without the infrastructure overhead. Focus on your data, your logic, your business value - we've got the rest covered.

**Start building your first pipeline in minutes:**

1. **Clone & Configure**: `cp env.example .env`
2. **Create Pipeline**: Add `src/pipelines/my_pipeline.py`
3. **Test Locally**: `./run.sh list && ./run.sh cron_run my_pipeline`
4. **Deploy**: `./deploy.sh user hostname`
5. **Monitor**: Check logs, data counts, and health metrics

**Welcome to the future of data processing! ğŸš€**