# Developer Guide

Guidelines for contributing to and extending the Data Processing Framework.

## ğŸš€ Development Setup

```bash
# Clone and setup
git clone <repository-url>
cd data-processor
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-test.txt
```

## ğŸ—ï¸ Project Structure

```
data-processor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                   # Framework foundation
â”‚   â””â”€â”€ pipelines/              # Pipeline system
â”‚       â”œâ”€â”€ base_pipeline.py   # Base pipeline classes
â”‚       â”œâ”€â”€ pipeline_registry.py # Centralized registry
â”‚       â””â”€â”€ tools/              # Generic utilities
â”‚           â”œâ”€â”€ data_utils.py
â”‚           â”œâ”€â”€ pagination_utils.py
â”‚           â”œâ”€â”€ backfill_utils.py
â”‚           â”œâ”€â”€ extractors/
â”‚           â”œâ”€â”€ loaders/
â”‚           â””â”€â”€ transformers/
â”œâ”€â”€ tests/                      # Test suite
â”œâ”€â”€ docs/                       # Documentation
â”œâ”€â”€ migrations/                 # Database migrations
â””â”€â”€ scripts/                    # Utility scripts
```

## ğŸ§© Core Components

### Configuration System
- **File**: `src/core/config.py`
- **Models**: `src/core/models.py`
- **Adding Config**: Add field to `FrameworkSettings` in `models.py`

### Logging System
- **File**: `src/core/logging.py`
- **Usage**: `log_with_timestamp("Message", "Component", "level")`

### Pipeline System
- **Base Classes**: `BasePipeline`, `MetabasePipeline`, `ClickHousePipeline`
- **Registry**: Centralized pipeline management
- **Tools**: Generic utilities for common tasks

## ğŸ”§ Creating New Pipelines

### Using Class-Based Approach

```python
from pipelines.base_pipeline import MetabasePipeline, ClickHousePipeline
from pipelines.tools.data_utils import prepare_datetime_columns, add_merge_metadata
from pipelines.pipeline_registry import pipeline_registry

class MyNewPipeline(MetabasePipeline, ClickHousePipeline):
    def __init__(self):
        super().__init__(
            name="my_new_pipeline",
            description="My new ETL pipeline",
            schedule="0 */6 * * *"
        )
        
        # Initialize components
        self.extractor = create_metabase_paginated_extractor(database_id=1)
        self.loader = create_clickhouse_replace_loader(
            table_name="my_table",
            key_columns=["id"],
            sort_column="updated_at"
        )
    
    async def extract(self) -> pd.DataFrame:
        query = "SELECT * FROM my_table WHERE updated_at >= '2024-01-01'"
        return await self.extractor.extract_from_query(query, "My Extractor")
    
    async def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        data = prepare_datetime_columns(data, {'created_at': 'created_at'})
        data = self._add_merge_metadata(data, 'success')
        return data
    
    async def load(self, data: pd.DataFrame) -> bool:
        return await self.loader(data)

# Register pipeline
def register_pipelines():
    pipeline = MyNewPipeline()
    pipeline_registry.register_pipeline(pipeline)
```

### Benefits
- **Inheritance**: Get common functionality from base classes
- **Generic Utilities**: Use pre-built data processing tools
- **Consistent Structure**: All pipelines follow the same pattern
- **Easy Testing**: Clear separation of concerns

## ğŸ› ï¸ Adding New Components

### New Extractor
```python
# Create src/pipelines/tools/extractors/my_extractor.py
import pandas as pd
from typing import Dict, Any, Callable, Awaitable

async def create_my_extractor(config: Dict[str, Any]) -> Callable[[], Awaitable[pd.DataFrame]]:
    async def extractor() -> pd.DataFrame:
        # Your extraction logic
        return pd.DataFrame()
    return extractor
```

### New Transformer
```python
# Create src/pipelines/tools/transformers/my_transformer.py
import pandas as pd
from typing import Callable

def create_my_transformer(transform_func: Callable, name: str) -> Callable[[pd.DataFrame], pd.DataFrame]:
    def transformer(data: pd.DataFrame) -> pd.DataFrame:
        return transform_func(data)
    return transformer
```

### New Loader
```python
# Create src/pipelines/tools/loaders/my_loader.py
import pandas as pd
from typing import Callable, Awaitable

async def create_my_loader(config: Dict[str, Any]) -> Callable[[pd.DataFrame], Awaitable[bool]]:
    async def loader(data: pd.DataFrame) -> bool:
        # Your loading logic
        return True
    return loader
```

## ğŸ§ª Testing

### Running Tests
```bash
./run.sh test                   # All tests
./run.sh test unit              # Unit tests only
./run.sh test integration       # Integration tests
./run.sh test --fast            # Fast tests only
```

### Test Structure
```
tests/
â”œâ”€â”€ unit/                       # Component tests
â”œâ”€â”€ integration/                # End-to-end tests
â”œâ”€â”€ performance/                # Performance tests
â””â”€â”€ results/                    # Test results
```

### Writing Tests
```python
import pytest
import pandas as pd
from pipelines.my_pipeline import MyPipeline

@pytest.mark.asyncio
async def test_pipeline_extract():
    pipeline = MyPipeline()
    data = await pipeline.extract()
    assert not data.empty
    assert 'id' in data.columns
```

## ğŸ“ Code Standards

### Python Style
- Use type hints for all functions
- Follow PEP 8 naming conventions
- Document all public methods
- Use async/await for I/O operations

### Pipeline Standards
- Inherit from appropriate base classes
- Use generic utilities when possible
- Handle errors gracefully
- Add comprehensive logging

### Testing Standards
- Write unit tests for all components
- Include integration tests for pipelines
- Aim for >80% code coverage
- Test error conditions

## ğŸš€ Deployment

### Local Testing
```bash
./run.sh check                  # Check dependencies
./run.sh test                   # Run tests
./run.sh list                   # List pipelines
```

### Production Deployment
```bash
./deploy.sh username hostname   # Deploy to server
```

## ğŸ” Debugging

### Logs
- **System**: `logs/system/application.log`
- **Jobs**: `logs/jobs/{job_name}.log`
- **Cron**: `logs/cron.log`

### Common Issues
- **Import Errors**: Check Python path and dependencies
- **Database Errors**: Verify ClickHouse configuration
- **Pipeline Errors**: Check logs for specific error messages

## ğŸ“š Resources

- [Quick Start Guide](./quick-start.md)
- [API Reference](./api-reference.md)
- [Architecture Overview](./architecture/README.md)
- [Migration Guide](./migration-guide.md)